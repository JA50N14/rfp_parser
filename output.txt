
﻿
RFP Script
Questions:
How many RFP's come through each month?
What is the process of how your team gets ahold of these RFP packages? (i.e. Emailed the package? Package uploaded to Salesforce by Salesperson?)
What do these RFP packages look like (i.e. several documents? 1 large pdf?, etc.)?
What document formats are contained in RFP packages (i.e. .pdf, .doc, .xlsx, etc.)?
What I need from Manasi:
 
A list of keywords you would like to find in a RFP doc? - DONE
 
A list of data you want to capture per RFQ Package (i.e. Client Name, Industry/Sector, etc.) - DONE
Requirements:
Parse RFP package provided from client (not yet filled out)
Write data into an excel/smartsheet output file OR database - Needs to be structured data.
Once a keyword or phrase is found, insert a true flag into output file for that keyword's column AND set the map to true for that key.
Insert the sentence of the keyword that was found into the output file
 
What happens if a keyword/phrase is found twice? Which sentence do we want? - If keyword/phrase comes up again, grab this sentence as well?
Need a config.json file for each RFP program run - Client Name, Industry/Sector (on SalesForce - Evelyn), Region/Country, Business Unit, Date RFP Received
Dashboarding with the structured data (i.e. Smartsheets? Excel?)
 
ex: Number of times each keyword/phrase appeared per Business Unit, Industry/Sector, Region/Country per year
 
Ideally have this dashboard populate automatically with output file, each time program is ran
 
Use smartsheet api to POST data???
Proposals to use tool (needs to be user friendly)
Pull data from 
 
User Functionality:
Installation:
Install the .exe file and place in their: C:\Users\firstname.lastname (if running via CLI) or anywhere if double-clicking
OR
API to pull files from SharePoint - Schedule a Cron job to run
Make metadata tag required on SP, and use these tags instead of a config.json file????
Write a file into the directory after program completion, which is used as a flag that that RFP has been parsed???
Auth Credentials needed (Tenant ID, Client ID, Client Secret) to authenticate to Microsoft Graph using app-only flow
Store credentials securely - Windows Credential Manager, Password-Protected Encrypted File
Running the Program:
User obtains all RFP files for a bid and puts them on their computer in a folder name "RFP_Package"
User completes config.json file and places in the "RFP_Package" dir (i.e. Client Name, Industry/Sector, Region/Country, Unique ID, etc.)
User places output file into "RFP_Package" dir???????
User runs the program - double click on the .exe file - to make cmd stay open when double-clicked, add fmt.Scanln() at end of main()
Constraints:
No smartsheet SDK for Go, so will need to execute http POST requests
Access to Microsoft Graph API?????
Implementation Details:
Constant Variables:
keyWords: key=keyword, val=int
fileTypes: file extensions (.pdf, .doc*, .xls*)
Check if dir named "RFP_Package" is located on desktop
If "RFP_Package" dir not present - print to stdout
Check for config.json file in "RFP_Package"
If config.json not present - print to stdout
Parse a config.json
If config.json missing data - print to stdout
Check the Unique ID to ensure duplicate data is not added to output file OR user type "y" to continue, "n" to quit
Loop through "RFP_Package" dir
If extension in fileTypes
Open & Read file - Stream into a temp file since you'll be using 3rd party libraries that likely expect a file path & avoid memory pressure
Parse file contents to search for keyWords
if keyWords found:
Increase count on KeyWords key
Get all chars from ". " to "."
Write to output file OR add to a []struct to be marshalled and POST in Smartsheet 
Go Routines:
Use for fetching files + http client pool to stream multiple files in parallel - save to temp files
Use for parsing files - open each temp file and parse. Use worker goroutines for parsing. Delete temp files after processing
Create an output file, use Smartsheet API, OR append to an existing file?????
Iterate through dir and sub-dirs., and open files with a .doc, .pdf, .xls, etc. extensions
Open the current file
Parse file
 
If keyword found: increase count in map struct / copy entire sentence & write into output file
Parser:
convert []string in struct to regex and then search for regex in each sentence
Split document into sentences - regex
Normalize sentence to lowercase
App Layout:
Libraries:
regexp
net/http
go get github.com/microsoftgraph/msgraph-sdk-go
go get github.com/Azure/azure-sdk-for-go/sdk/azidentity
go get github.com/zalando/go-keyring
Constants:
Phrase struct
file_extensions (.doc*, .xls*, .pdf)
Client Secret (Graph API)
Client ID (Graph API)
Tenant ID (Graph API)
Bearer Token (Smartsheet API)
GET Requests - Microsoft Graph API:
Hit API to get list of all docs and sub-dirs. - iterate through (recursively call func if a sub-folder) to see if .pdf, etc. If .pdf, .doc, etc., call API to download func traverseFolder
1. Request to url to get list of root dirs.
2. Loop through list of root dirs.
3. Process each root dir - request to each root dir to get list of all files & sub-dirs. - processRootFolder(item.id) <-will be a recursive function
4. For each file/dir in RFP root: -if dir -> recursively call traverseFolder
if file = __processed.txt exists -> stop (means this dir has already been processed)
if file && matchesExtension() -> downloadAndProcessFile()
5. After RFP dir done processing - create a __processed.txt file and POST into RFP dir
2. build a result struct
3. decode response body
3. loop through result
if dir -> recursively call traverseFolder
if file = flagFile -> stop (means this dir has already been parsed)
if file && matchesExtension() -> downloadAndProcessFile()
4. downloadAndProcessFile() - build url, make new request, set bearer token in header, create temp file, stream bytes into temp file, pass temp file path & file extension into "Parser"
Parser:
1.Stream byte data from file into memory
2.Based on extension, pass byte data into either pdf parser, xlsx parser, or docx parser - will read the binary bytes from memory and expose text content
3.Use regex to create a slice of sentences
4.Loop through sentences and match phrases
Parse Build Instructions/POC:
Build a data structure to hold phrases, found, sentences – Add ColumnID which is the column id in smartsheet
OR precompiled regex patterns to avoid recompiling for every sentence
Create a JSON file with all 52 phrases
Create a SmartsheetRow struct:
Constants:
rootURL
KpiTracker struct
file_extensions
Bearer Token (Smartsheet API)
func main{}
Instantiate a []SmartsheetRow struct
Call traverseRfpPackages(root string)
Call kpiTrackerToSmartsheetRows()
Create a logger (lib - log/slog)
func (cfg *apiConfig) travserseRfpPackages() Error {}
loop through document library/root directory
Call rfpProcessedCompleteCheck(root string) Boolean{}
If RFP Package processed, continue
Create a instance of []SmartsheetRow
Call traverseRfpPackage()
func rfpProcessedCompleteCheck(rfpRoot string) (bool, error) {}
global reporting initiative
iterates through the RFP package to see if a __processed file exists
Alternative methods when hooked into Microsoft Graph API:
Store directory ID’s in a db, key-value store, or json file in blob storage
SharePoint metadata – filter by “Processed: Yes/No” 
Need another function to write “Yes” at end of RFP Package processing
func (cfg *apiConfig) traverseRfpPackage(rfpPackage string, SmartsheetRows []SmartsheetRow) Error{} 
Call loadKpiTracker()
Call compileRegexStrings()
Create a “explicit stack-based iteration” instead of recursion to prevent stack overflow on deep folder structures
put root into a stack (stack := []string{rfpRoot})
loop while stack has items
current dir
remove current from stack
loop through current dir
if sub-dir, append to stack
if file && matchesExtension() -> downloadAndProcessFile()
Call func (cfg *apiConfig) kpiTrackerToSmartsheetRows(tracker *[]KpiTracker, SmartsheetRows []SmartsheetRow)
Func loadKpiTracker() []KpiTracker{} 
Function to instantiate a []KpiTracker struct and load the kpi_tracker.json data into it.
func downloadAndProcessFile(file string) Error {} 
stream file bytes into memory (consider stream parsing if supported by library to avoid exceeding memory limits).
Call the appropriate parser (i.e. parserPdf, parserWord, parserExcel)
Call kpiParser()
Call postToSmartsheets()
Create functions for: parserPdf, parserWord, parserExcel
parse document into memory using the proper library
IFRS
func (tracker *[]KpiTracker) kpiParser(data []byte) Error {}
parser (pdf, word doc) will typically split via new line
clean – replace non-breaking spaces with normal spaces
clean – trim bullets, dashes, etc. From start of line
clean – normalize spaces
clean – lowercase
create a slice of sentences
loop through each sentence
loop through Variations attribute on kpiTracker struct
if Variation found in sentence, mark as Found and append sentence to Sentences attribute
func (smartsheetRows *[]SmartsheetRow) kpiTrackerToSmartsheetRows(tracker *[]KpiTracker) Error {}
Map the ColumnID, Found, and Sentences attributes of each struct to a SmartsheetRow struct
func (smartsheetRows *[]SmartsheetRow) postToSmartsheets() Error {}
create a http client
marshal into json
send POST request
Create a logging mechanism 
POST Requests - Smartsheets:
Logging:
sustainable development goals
PDF - could be a pic in PDF format - cannot parse text. When reading file, there may not be text stream to parse against.
1. Parsing data and exporting the information - get access to 10 RFP - Use this to ingest - Parse - Stream into memory - save to json - based on how this parser works.(hook into Proposal Doc library) - In-memory is better
2. Then talk to Robert to pull from SP
3. Then push SS 
Parsing:
Split by \n
If KPI found: 
find start idx and end idx of KPI
find “. capitalLetter“ before start idx
If not found, set idx 0 as starting point
find “. CapitalLetter” after end idx
if not found, take len(text[i]) as end point
Grab everything between starting idx and ending idx that is surrounding the KPI
66j3CXwdWCPw1mPLIOvlXRreiDhKJVgpDo4KC
